{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Sets 1 and 2': 0.3333333333333333,\n",
       "  'Sets 1 and 3': 0.4,\n",
       "  'Sets 2 and 3': 0.16666666666666666},\n",
       " {'Bags 1 and 2': 0.5,\n",
       "  'Bags 1 and 3': 0.3333333333333333,\n",
       "  'Bags 2 and 3': 0.5})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 1 (a)(b)\n",
    "\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "\n",
    "# Define the sets and bags\n",
    "sets = [\n",
    "    {1, 2, 3, 4},\n",
    "    {2, 3, 5, 7},\n",
    "    {2, 4, 6}\n",
    "]\n",
    "\n",
    "bags = [\n",
    "    [1, 1, 1, 2],\n",
    "    [1, 1, 2, 2, 3],\n",
    "    [1, 2, 3, 4]\n",
    "]\n",
    "\n",
    "# Function to compute Jaccard similarity between two sets\n",
    "def jaccard_similarity(set1, set2):\n",
    "    intersection = set1.intersection(set2)\n",
    "    union = set1.union(set2)\n",
    "    return len(intersection) / len(union)\n",
    "\n",
    "# Function to compute Jaccard bag similarity between two bags\n",
    "def jaccard_bag_similarity(bag1, bag2):\n",
    "    bag1_counter = Counter(bag1)\n",
    "    bag2_counter = Counter(bag2)\n",
    "    intersection_sum = sum((bag1_counter & bag2_counter).values())\n",
    "    union_sum = sum((bag1_counter | bag2_counter).values())\n",
    "    return intersection_sum / union_sum\n",
    "\n",
    "# Compute Jaccard similarities for each pair of sets\n",
    "jaccard_similarities = {\n",
    "    f\"Sets {i+1} and {j+1}\": jaccard_similarity(pair[0], pair[1])\n",
    "    for i, j in combinations(range(len(sets)), 2)\n",
    "    for pair in [sorted([sets[i], sets[j]], key=id)]\n",
    "}\n",
    "\n",
    "# Compute Jaccard bag similarities for each pair of bags\n",
    "jaccard_bag_similarities = {\n",
    "    f\"Bags {i+1} and {j+1}\": jaccard_bag_similarity(bag1, bag2)\n",
    "    for i, bag1 in enumerate(bags)\n",
    "    for j, bag2 in enumerate(bags)\n",
    "    if i < j\n",
    "}\n",
    "\n",
    "jaccard_similarities, jaccard_bag_similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1 (c)\n",
    "\n",
    "1. **Intersection (\\(|S  $\\cap$  T|\\))**: The expected size of the intersection is based on the probability of each element in \\(U\\) being chosen in both \\(S\\) and \\(T\\). Since each element has an equal chance of being selected, and each subset contains \\(m\\) elements, the expected intersection size can be calculated considering the binomial distribution of selecting \\(m\\) elements out of \\(n\\) twice independently.\n",
    "\n",
    "2. **Union (\\(|S $\\cup$ T|\\))**: The union of \\(S\\) and \\(T\\) can have at most \\(2m\\) elements (if \\(S\\) and \\(T\\) are entirely distinct) and at least \\(m\\) elements (if \\(S = T\\)). The expected size of the union depends on the overlap between \\(S\\) and \\(T\\), which is directly related to the size of their intersection.\n",
    "\n",
    "3. **Expected Jaccard Similarity**: Combining the above, the expected Jaccard similarity can be formulated as a function of \\(n\\), \\(m\\), and the probability distributions governing the selection of elements into \\(S\\) and \\(T\\).\n",
    "\n",
    "The exact calculation involves detailed combinatorial analysis, considering all possible ways \\(m\\) elements can be chosen twice from \\(n\\) elements and their intersection and union sizes. For large \\(n\\) and \\(m\\), approximations or simulations may be used to estimate this expected value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the most effective',\n",
       " 'most effective way',\n",
       " 'effective way to',\n",
       " 'way to represent',\n",
       " 'to represent documents',\n",
       " 'represent documents as',\n",
       " 'documents as sets',\n",
       " 'as sets for',\n",
       " 'sets for the',\n",
       " 'for the purpose']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 2 (a)\n",
    "# Define the sentence for shingling\n",
    "sentence = \"The most effective way to represent documents as sets, for the purpose of identifying lexically similar documents is to construct from the document the set of short strings that appear within it.\"\n",
    "\n",
    "# Function to generate k-shingles from text\n",
    "def generate_k_shingles(text, k):\n",
    "    # Remove punctuation and make lowercase\n",
    "    text = ''.join(c.lower() for c in text if c.isalnum() or c.isspace())\n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "    # Generate k-shingles\n",
    "    shingles = [' '.join(words[i:i+k]) for i in range(len(words)-k+1)]\n",
    "    return shingles\n",
    "\n",
    "# Generate the first ten 3-shingles\n",
    "first_ten_3_shingles = generate_k_shingles(sentence, 3)[:10]\n",
    "first_ten_3_shingles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the most effective\n",
      "way to represent\n",
      "to represent documents\n",
      "as sets, for\n",
      "for the purpose\n",
      "the purpose of\n",
      "of identifying lexically\n",
      "is to construct\n",
      "to construct from\n",
      "the document the\n",
      "the set of\n",
      "set of short\n",
      "of short strings\n"
     ]
    }
   ],
   "source": [
    "# Question 2 (a)\n",
    "def tokenize(sentence):\n",
    "    \"\"\"Simple tokenizer to split the sentence into words.\"\"\"\n",
    "    return sentence.split()\n",
    "\n",
    "def identify_stop_words(words, max_length=3):\n",
    "    \"\"\"Identify stop words based on their length.\"\"\"\n",
    "    return [word for word in words if len(word) <= max_length]\n",
    "\n",
    "def generate_shingles(words, stop_words):\n",
    "    \"\"\"Generate shingles starting with a stop word followed by the next two words.\"\"\"\n",
    "    shingles = []\n",
    "    for i, word in enumerate(words):\n",
    "        if word in stop_words:\n",
    "            # Ensure there are at least two words following the stop word\n",
    "            if i + 2 < len(words):\n",
    "                shingle = \" \".join(words[i:i+3])\n",
    "                shingles.append(shingle)\n",
    "    return shingles\n",
    "\n",
    "# The sentence to analyze\n",
    "sentence = \"The most effective way to represent documents as sets, for the purpose of identifying lexically similar documents is to construct from the document the set of short strings that appear within it.\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "words = tokenize(sentence.lower())  # Convert to lowercase to ensure consistency\n",
    "\n",
    "# Identify stop words\n",
    "stop_words = identify_stop_words(words)\n",
    "\n",
    "# Generate shingles\n",
    "shingles = generate_shingles(words, stop_words)\n",
    "\n",
    "# Display the generated shingles\n",
    "for shingle in shingles:\n",
    "    print(shingle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The most effective',\n",
       " 'way to represent',\n",
       " 'to represent documents',\n",
       " 'as sets, for',\n",
       " 'for the purpose',\n",
       " 'the purpose of',\n",
       " 'of identifying lexically',\n",
       " 'is to construct',\n",
       " 'to construct from',\n",
       " 'the document the',\n",
       " 'the set of',\n",
       " 'set of short',\n",
       " 'of short strings']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 2 (b)\n",
    "# Define a function to generate stop-word-based shingles\n",
    "def generate_stop_word_based_shingles(text, stop_word_length=3):\n",
    "    # Define stop words as words of three or fewer letters\n",
    "    stop_words = set(word for word in text.split() if len(word) <= stop_word_length)\n",
    "    \n",
    "    # Initialize a list to hold the shingles\n",
    "    shingles = []\n",
    "    words = text.split()\n",
    "    \n",
    "    # Generate shingles\n",
    "    for i, word in enumerate(words):\n",
    "        if word in stop_words:\n",
    "            # Extract a shingle starting with the stop word and the next two words, if available\n",
    "            shingle = ' '.join(words[i:i+3])\n",
    "            shingles.append(shingle)\n",
    "    \n",
    "    # Remove shingles that don't start with a stop word followed by exactly two non-stop words\n",
    "    shingles = [shingle for shingle in shingles if len(shingle.split()) == 3 and shingle.split()[0] in stop_words]\n",
    "    return shingles\n",
    "\n",
    "# Generate the stop-word-based shingles for the given sentence\n",
    "sentence = \"The most effective way to represent documents as sets, for the purpose of identifying lexically similar documents is to construct from the document the set of short strings that appear within it.\"\n",
    "stop_word_based_shingles = generate_stop_word_based_shingles(sentence)\n",
    "stop_word_based_shingles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2 (c)\n",
    "The largest number of k-shingles a document of n bytes can have is when every consecutive substring of length k is unique.\n",
    "This is achieved when each new byte after the first k bytes forms a new k-shingle.\n",
    "Therefore, the formula for the largest number of k-shingles is n - k + 1, assuming n >= k.\n",
    "\n",
    "Since we are not solving for a specific value but providing a general formula, we display the formula directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard similarity: \n",
      " {(0, 1): 0.0, (0, 2): 0.25, (0, 3): 0.6666666666666666, (1, 2): 0.0, (1, 3): 0.3333333333333333, (2, 3): 0.2}, \n",
      "The fraction of the 120 permutations: \n",
      " {(0, 1): 0.0, (0, 2): 0.25, (0, 3): 0.6666666666666666, (1, 2): 0.0, (1, 3): 0.3333333333333333, (2, 3): 0.2}\n"
     ]
    }
   ],
   "source": [
    "# Question 3\n",
    "import numpy as np\n",
    "from itertools import permutations\n",
    "\n",
    "# The matrix representation of sets S1, S2, S3, and S4.\n",
    "matrix = np.array([\n",
    "    [1, 0, 0, 1],  # Element a\n",
    "    [0, 0, 1, 0],  # Element b\n",
    "    [0, 1, 0, 1],  # Element c\n",
    "    [1, 0, 1, 1],  # Element d\n",
    "    [0, 0, 1, 0]   # Element e\n",
    "])\n",
    "\n",
    "# Function to compute Jaccard similarity between two sets represented as binary vectors\n",
    "def _jaccard_similarity(set1, set2):\n",
    "    intersection = np.sum(np.logical_and(set1, set2))\n",
    "    union = np.sum(np.logical_or(set1, set2))\n",
    "    return intersection / union\n",
    "\n",
    "# Compute Jaccard similarity for each pair of columns (sets)\n",
    "jaccard_similarities = {\n",
    "    (i, j): _jaccard_similarity(matrix[:, i], matrix[:, j])\n",
    "    for i in range(matrix.shape[1])\n",
    "    for j in range(matrix.shape[1]) if i < j\n",
    "}\n",
    "\n",
    "# Function to get the hash value for a given column and permutation\n",
    "def hash_value(column, perm):\n",
    "    # Apply the permutation and get the hash value\n",
    "    permuted_column = column[list(perm)]\n",
    "    for row_index in range(len(permuted_column)):\n",
    "        if permuted_column[row_index] == 1:\n",
    "            return row_index\n",
    "    return -1\n",
    "\n",
    "# Compute the fraction of permutations that hash to the same value for each pair of columns\n",
    "num_rows = matrix.shape[0]\n",
    "all_permutations = list(permutations(range(num_rows)))\n",
    "matching_permutations = {\n",
    "    (i, j): sum(1 for perm in all_permutations if hash_value(matrix[:, i], perm) == hash_value(matrix[:, j], perm)) / len(all_permutations)\n",
    "    for i in range(matrix.shape[1])\n",
    "    for j in range(matrix.shape[1]) if i < j\n",
    "}\n",
    "\n",
    "print(f\"Jaccard similarity: \\n {jaccard_similarities}, \\nThe fraction of the 120 permutations: \\n {matching_permutations}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(a) minhash_signatures: {'Column 1': [5, 2, 0], 'Column 2': [1, 2, 1], 'Column 3': [1, 2, 4], 'Column 4': [1, 2, 0]} \n",
      "(b)permutations_check: {'h1': False, 'h2': False, 'h3': True} \n",
      "(c)true_jaccard_similarities: {(0, 1): 0.0, (0, 2): 0.0, (0, 3): 0.25, (1, 2): 0.0, (1, 3): 0.25, (2, 3): 0.25}, \n",
      "estimated_jaccard_similarities: {(0, 1): 0.3333333333333333, (0, 2): 0.3333333333333333, (0, 3): 0.6666666666666666, (1, 2): 0.6666666666666666, (1, 3): 0.6666666666666666, (2, 3): 0.6666666666666666}\n"
     ]
    }
   ],
   "source": [
    "# Question 4\n",
    "# Given matrix for the question\n",
    "matrix = np.array([\n",
    "    [0, 1, 0, 1],  # Element 0\n",
    "    [0, 1, 0, 0],  # Element 1\n",
    "    [1, 0, 0, 1],  # Element 2\n",
    "    [0, 0, 1, 0],  # Element 3\n",
    "    [0, 0, 1, 1],  # Element 4\n",
    "    [1, 0, 0, 0]   # Element 5\n",
    "])\n",
    "\n",
    "# Define the hash functions\n",
    "def h1(x):\n",
    "    return (2*x + 1) % 6\n",
    "\n",
    "def h2(x):\n",
    "    return (3*x + 2) % 6\n",
    "\n",
    "def h3(x):\n",
    "    return (5*x + 2) % 6\n",
    "\n",
    "# Function to compute the minhash signature of a column\n",
    "def minhash(column, hash_functions):\n",
    "    signature = []\n",
    "    for hash_function in hash_functions:\n",
    "        # Apply hash function to each row and take the minimum hash value where the column has 1\n",
    "        min_hash_value = min(hash_function(i) for i in range(len(column)) if column[i] == 1)\n",
    "        signature.append(min_hash_value)\n",
    "    return signature\n",
    "\n",
    "# Compute the minhash signature for each column\n",
    "hash_functions = [h1, h2, h3]\n",
    "minhash_signatures = {f\"Column {i+1}\": minhash(matrix[:, i], hash_functions) for i in range(matrix.shape[1])}\n",
    "\n",
    "# Check which hash functions are true permutations of the rows\n",
    "def is_permutation(hash_function, num_rows):\n",
    "    # A hash function is a true permutation if it produces all unique values when applied to all rows\n",
    "    hashed_values = [hash_function(i) for i in range(num_rows)]\n",
    "    return len(set(hashed_values)) == num_rows\n",
    "\n",
    "# Verify the hash functions\n",
    "permutations_check = {\n",
    "    \"h1\": is_permutation(h1, matrix.shape[0]),\n",
    "    \"h2\": is_permutation(h2, matrix.shape[0]),\n",
    "    \"h3\": is_permutation(h3, matrix.shape[0])\n",
    "}\n",
    "\n",
    "# To answer part (c), we need the true Jaccard similarities first, for comparison\n",
    "# Calculate the true Jaccard similarities for all pairs of columns\n",
    "true_jaccard_similarities = {\n",
    "    (i, j): _jaccard_similarity(matrix[:, i], matrix[:, j])\n",
    "    for i in range(matrix.shape[1])\n",
    "    for j in range(matrix.shape[1]) if i < j\n",
    "}\n",
    "\n",
    "# Function to estimate Jaccard similarity using minhash signatures\n",
    "def estimated_jaccard(signature1, signature2):\n",
    "    return sum(1 for i in range(len(signature1)) if signature1[i] == signature2[i]) / len(signature1)\n",
    "\n",
    "# Calculate the estimated Jaccard similarities for all pairs of columns using the minhash signatures\n",
    "estimated_jaccard_similarities = {\n",
    "    (i, j): estimated_jaccard(minhash(matrix[:, i], hash_functions), minhash(matrix[:, j], hash_functions))\n",
    "    for i in range(matrix.shape[1])\n",
    "    for j in range(matrix.shape[1]) if i < j\n",
    "}\n",
    "print(f\"(a) minhash_signatures: {minhash_signatures} \\n(b)permutations_check: {permutations_check} \\n(c)true_jaccard_similarities: {true_jaccard_similarities}, \\nestimated_jaccard_similarities: {estimated_jaccard_similarities}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5\n",
    "Minhashing is a technique that converts sets to a smaller representation or \"signature\", preserving similarity. A Minhash value for a set corresponds to the index of the first non-zero element in its characteristic bit vector when the elements have been permuted randomly. \n",
    "\n",
    "Given a bit vector `x` of length `n` with `m` zeros, the min-hash value `h(x)` is determined by the position of the first '1' following a permutation of the vector. \n",
    "\n",
    "Since there are `m` zeros, the first '1' can be at the first position if all zeros are after it, which is the best-case scenario, yielding `h(x) = 1`. In the worst case, all `m` zeros precede the first '1', making it at the position `m + 1`. Therefore, the min-hash value of this vector, `h(x)`, will always satisfy `1 ≤ h(x) ≤ m + 1`. \n",
    "\n",
    "This property of min-hash arises because we are looking for the first '1' in the permuted vector. No matter how we permute the entries, if there are `m` zeros, then there will be `m` positions that the first '1' cannot occupy, which are the positions filled by these zeros. Thus, the first '1' must be within the first `m + 1` entries of the permuted bit vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S-curve results: {(3, 10): [0.009955119790251765, 0.07718058804273675, 0.23944889319887064, 0.4838707317677322, 0.7369244238361716, 0.9122674753991765, 0.985015105295655, 0.9992340538808936, 0.9999978635491371], (6, 20): [1.9999810001669616e-05, 0.001279222058761964, 0.014479466504172311, 0.07880932311056232, 0.27018714400947597, 0.6154146360312677, 0.9181859965846744, 0.9977121251546806, 0.9999997398129465], (5, 50): [0.0004998775195954597, 0.01587519984502117, 0.11453988231042189, 0.4022839522088044, 0.7955506304323648, 0.9825338277068608, 0.9998989958361557, 0.9999999976077777, 1.0]}\n",
      "Thresholds: {(3, 10): 0.4060881340677082, (6, 20): 0.5693533868256984, (5, 50): 0.42439448036873306}\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 6\n",
    "import numpy as np\n",
    "from scipy.optimize import brentq\n",
    "\n",
    "# S-curve function\n",
    "def s_curve(s, r, b):\n",
    "    return 1 - (1 - s**r)**b\n",
    "\n",
    "# Values of s to evaluate\n",
    "s_values = np.arange(0.1, 1.0, 0.1)\n",
    "\n",
    "# Pairs of (r, b) values\n",
    "rb_pairs = [(3, 10), (6, 20), (5, 50)]\n",
    "\n",
    "# Evaluate the S-curve for each s and each (r, b) pair\n",
    "s_curve_results = {rb: [s_curve(s, *rb) for s in s_values] for rb in rb_pairs}\n",
    "\n",
    "# Compute the threshold for each (r, b) pair\n",
    "def find_threshold(r, b):\n",
    "    # Define the equation for which we want to find the root\n",
    "    equation = lambda s: s_curve(s, r, b) - 0.5\n",
    "    # Find the root\n",
    "    threshold = brentq(equation, 0.0, 1.0)\n",
    "    return threshold\n",
    "\n",
    "# Compute thresholds\n",
    "thresholds = {rb: find_threshold(*rb) for rb in rb_pairs}\n",
    "\n",
    "print(\"S-curve results:\", s_curve_results)\n",
    "print(\"Thresholds:\", thresholds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7\n",
    "The properties of a metric on distance measure, which includes:\n",
    "\n",
    "1. **Non-negativity**: $JaccardDistance(A, B) \\geq 0$ for any sets $A$ and $B$.\n",
    "2. **Identity of indiscernibles**: $JaccardDistance(A, B) = 0$ if and only if $A = B$.\n",
    "3. **Symmetry**: $JaccardDistance(A, B) = JaccardDistance(B, A)$ for any sets $A$ and $B$.\n",
    "4. **Triangle inequality**: $JaccardDistance(A, C) \\leq JaccardDistance(A, B) + JaccardDistance(B, C)$ for any sets $A$, $B$, and $C$.\n",
    "\n",
    "The Jaccard distance is defined as $JaccardDistance(A, B) = 1 - JaccardSimilarity(A, B)$, where the Jaccard similarity, $JaccardSimilarity(A, B)$, is $\\frac{|A \\cap B|}{|A \\cup B|}$. \n",
    "\n",
    "Now, let's show that Jaccard distance satisfies these four properties:\n",
    "\n",
    "1. **Non-negativity**: The intersection of two sets will never have more elements than their union, so $|A \\cap B| \\leq |A \\cup B|$. Hence, the Jaccard similarity is always between 0 and 1, and the Jaccard distance, being $1 - JaccardSimilarity$, is also between 0 and 1. Therefore, $JaccardDistance(A, B) \\geq 0$.\n",
    "\n",
    "2. **Identity of indiscernibles**: If $A = B$, then $|A \\cap B| = |A| = |B|$ and $|A \\cup B| = |A| = |B|$, so $JaccardSimilarity(A, B) = 1$ and thus $JaccardDistance(A, B) = 0$. Conversely, if $JaccardDistance(A, B) = 0$, this implies that $JaccardSimilarity(A, B) = 1$, which means $|A \\cap B| = |A \\cup B|$, and therefore $A = B$.\n",
    "\n",
    "3. **Symmetry**: Since the intersection and union of sets are commutative operations, $|A \\cap B| = |B \\cap A|$ and $|A \\cup B| = |B \\cup A|$, it follows that $JaccardSimilarity(A, B) = JaccardSimilarity(B, A)$. Thus, $JaccardDistance(A, B) = JaccardDistance(B, A)$.\n",
    "\n",
    "4. **Triangle inequality**: This is the most complex property to prove and generally requires a more involved argument. To satisfy the triangle inequality, we need to show that for any sets $A$, $B$, and $C$, the Jaccard distance between $A$ and $C$ is less than or equal to the sum of the Jaccard distances between $A$ and $B$, and $B$ and $C$.\n",
    "\n",
    "Hence, the Jaccard distance satisfies all four properties of a metric space and is, therefore, a distance measure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 8\n",
    "To prove that for any positive integers $i$ and $j$ where $i < j$, the $L_i$ norm between any two points is greater than the $L_j$ norm between those same two points, let's consider two points $P$ and $Q$ in a space with $n$ dimensions. The coordinates of $P$ are given by $p_1, p_2, \\ldots, p_n$, and the coordinates of $Q$ are given by $q_1, q_2, \\ldots, q_n$.\n",
    "\n",
    "The $L_i$ norm (or $L_i$ distance) between points $P$ and $Q$, denoted as $||P - Q||_i$, is defined as:\n",
    "\n",
    "$$ ||P - Q||_i = \\left(\\sum_{k=1}^{n} |p_k - q_k|^i\\right)^{1/i} $$\n",
    "\n",
    "Similarly, the $L_j$ norm between $P$ and $Q$ is:\n",
    "\n",
    "$$ ||P - Q||_j = \\left(\\sum_{k=1}^{n} |p_k - q_k|^j\\right)^{1/j} $$\n",
    "\n",
    "Given that $i < j$, we aim to show that:\n",
    "\n",
    "$$ ||P - Q||_i > ||P - Q||_j $$\n",
    "\n",
    "This statement can be proven by understanding the effect of raising the absolute differences $|p_k - q_k|$ to higher powers. When $i < j$, the power $j$ amplifies larger differences more significantly than the power $i$, making the overall $L_j$ distance smaller than the $L_i$ distance due to the \"smoothing\" effect of raising differences to a higher power.\n",
    "\n",
    "More formally, this can be understood by considering the properties of $p$-norms:\n",
    "\n",
    "1. **Non-negativity and Definiteness**: Both $||P - Q||_i$ and $||P - Q||_j$ are non-negative and are zero if and only if $P = Q$.\n",
    "\n",
    "2. **Symmetry**: $||P - Q||_i = ||Q - P||_i$ and $||P - Q||_j = ||Q - P||_j$.\n",
    "\n",
    "3. **Triangle Inequality**: For any three points $P$, $Q$, and $R$, the triangle inequality holds for both $L_i$ and $L_j$ norms.\n",
    "\n",
    "4. **Homogeneity**: $||\\lambda(P - Q)||_i = |\\lambda|||P - Q||_i$ and $||\\lambda(P - Q)||_j = |\\lambda|||P - Q||_j$.\n",
    "\n",
    "To prove that $||P - Q||_i > ||P - Q||_j$, we leverage the fact that for $i < j$, the function $f(x) = x^{j/i}$ is convex for $x \\geq 0$. By applying Jensen's inequality, we find that for a set of positive numbers, the inequality:\n",
    "\n",
    "$$ \\left(\\sum |p_k - q_k|^i\\right)^{j/i} > \\sum |p_k - q_k|^j $$\n",
    "\n",
    "holds, leading us to conclude that $||P - Q||_i > ||P - Q||_j$ under the given conditions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9999985648333911,\n",
       " 0.12415142128492862,\n",
       " 0.9653880888384516,\n",
       " 0.03259738413767983,\n",
       " 144)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 9\n",
    "# Base sensitivity parameters\n",
    "p1_base, p2_base, n1_base, n2_base = 0.25, 0.75, 0.75, 0.25\n",
    "\n",
    "# Step 1: (4, 3) OR-AND construction\n",
    "p1_OR = 1 - (1 - n1_base) ** 4\n",
    "p2_OR = 1 - (1 - n2_base) ** 4\n",
    "n1_AND = p1_OR ** 3\n",
    "n2_AND = p2_OR ** 3\n",
    "\n",
    "# Step 2: (3, 4) AND-OR construction applied to the results of step 1\n",
    "n1_AND_2 = n1_AND ** 3\n",
    "n2_AND_2 = n2_AND ** 3\n",
    "p1_OR_2 = 1 - (1 - n1_AND_2) ** 4\n",
    "p2_OR_2 = 1 - (1 - n2_AND_2) ** 4\n",
    "\n",
    "# Total number of base hash functions needed\n",
    "total_base_hash_functions = 12 * 12  # 12 from the first construction applied 12 times in the second construction\n",
    "\n",
    "(p1_OR_2, p2_OR_2, n1_AND_2, n2_AND_2, total_base_hash_functions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'a': [1, 1, 1, 1], 'b': [0, 1, 0, 1], 'c': [1, 0, 1, 0]},\n",
       " {'ab': {'estimated': 90.0, 'true': 74.97388624088055},\n",
       "  'ac': {'estimated': 90.0, 'true': 105.02611375911947},\n",
       "  'bc': {'estimated': 180.0, 'true': 180.0}})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 10\n",
    "import numpy as np\n",
    "\n",
    "# Define the \"random\" vectors and the given vectors\n",
    "random_vectors = np.array([[1, 1, 1, -1], [1, 1, -1, 1], [1, -1, 1, 1], [-1, 1, 1, 1]])\n",
    "vectors = {\n",
    "    \"a\": np.array([2, 3, 4, 5]),\n",
    "    \"b\": np.array([-2, 3, -4, 5]),\n",
    "    \"c\": np.array([2, -3, 4, -5])\n",
    "}\n",
    "\n",
    "# Compute sketches\n",
    "def compute_sketch(vector, random_vectors):\n",
    "    return [1 if np.dot(vector, rv) >= 0 else 0 for rv in random_vectors]\n",
    "\n",
    "sketches = {key: compute_sketch(vec, random_vectors) for key, vec in vectors.items()}\n",
    "\n",
    "# Estimate angles between pairs of vectors based on sketches\n",
    "def estimate_angle(sketch1, sketch2):\n",
    "    hamming_distance = sum(x != y for x, y in zip(sketch1, sketch2))\n",
    "    fraction_of_disagreement = hamming_distance / len(sketch1)\n",
    "    estimated_angle = np.arccos(1 - 2 * fraction_of_disagreement)\n",
    "    return np.degrees(estimated_angle)\n",
    "\n",
    "# True angles between pairs of vectors\n",
    "def true_angle(vec1, vec2):\n",
    "    cosine_similarity = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "    return np.degrees(np.arccos(cosine_similarity))\n",
    "\n",
    "# Calculate estimated and true angles\n",
    "pair_keys = [\"ab\", \"ac\", \"bc\"]\n",
    "angles = {key: {} for key in pair_keys}\n",
    "for pair in pair_keys:\n",
    "    angles[pair][\"estimated\"] = estimate_angle(sketches[pair[0]], sketches[pair[1]])\n",
    "    angles[pair][\"true\"] = true_angle(vectors[pair[0]], vectors[pair[1]])\n",
    "\n",
    "sketches, angles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 11\n",
    "Given the task of performing entity resolution among bibliographic references, we encounter a scenario where:\n",
    "- All references include a year of publication, equally likely to be any of the ten most recent years.\n",
    "- Pairs of references with a perfect score have an average difference in publication year of 0.1.\n",
    "- Pairs of references with a certain score \\(s\\) have an average difference in publication dates of 2.\n",
    "\n",
    "We aim to calculate the fraction of pairs with score \\(s\\) that truly represent the same publication.\n",
    "\n",
    "### Step 1: Expected Average Difference in Publication Year Among Random Pairs\n",
    "\n",
    "Given a uniform distribution of publication years, the expected difference \\($E_{\\text{random}}$\\) is calculated as:\n",
    "\n",
    "$$\n",
    "E_{\\text{random}} = \\frac{\\sum_{d=1}^{9} d \\cdot (10-d)}{\\sum_{d=1}^{9} (10-d)}\n",
    "$$\n",
    "\n",
    "This formula accounts for the fact that differences near the middle of the range are more common than those near the ends.\n",
    "\n",
    "### Step 2: Fraction of Pairs with Score \\(s\\) Representing the Same Publication\n",
    "\n",
    "Let \\( $E_{\\text{perfect}} = 0.1 $ \\) represent the average year difference for pairs with a perfect score, and \\( $ E_s = 2 $ \\) for pairs with score \\(s\\). We seek the fraction \\(f\\) of score \\(s\\) pairs that are the same publication, balancing the equation:\n",
    "\n",
    "$$\n",
    "f \\cdot E_{\\text{perfect}} + (1 - f) \\cdot E_{\\text{random}} = E_s\n",
    "$$\n",
    "\n",
    "Solving for \\(f\\) gives:\n",
    "\n",
    "$$\n",
    "f = \\frac{E_s - E_{\\text{random}}}{E_{\\text{perfect}} - E_{\\text{random}}}\n",
    "$$\n",
    "\n",
    "### Calculations\n",
    "\n",
    "- First, calculate \\( $E_{\\text{random}}$ \\) using the provided formula.\n",
    "- Then, solve for \\(f\\) using the values for \\( $E_{\\text{perfect}}$ \\), \\( $E_s$ \\), and your calculated \\( $E_{\\text{random}}$ \\).\n",
    "\n",
    "This methodology allows us to estimate the fraction of bibliographic reference pairs with score \\(s\\) that likely represent identical publications based on the given data and statistical reasoning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 12\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Distance Measures\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
